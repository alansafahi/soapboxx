import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export interface ModerationResult {
  flagged: boolean;
  priority: 'low' | 'medium' | 'high' | 'critical';
  violations: string[];
  reason: string;
  confidence: number;
  actionRequired: 'none' | 'review' | 'hide' | 'remove';
}

export async function analyzeContentForViolations(content: string, contentType: string): Promise<ModerationResult> {
  try {
    const prompt = `
You are a content moderation AI for a faith-based community platform. Analyze this ${contentType} content for violations and respond with JSON only.

Content to analyze: "${content}"

Evaluate for these violation categories with their priority levels:

HIGH PRIORITY (immediate action required):
- Harassment or bullying (targeting, intimidating behavior)
- Inappropriate content (sexual, violent, adult language)
- Hate speech or discrimination (faith, race, gender based)
- Blasphemous or profane misuse of Scripture
- Predatory behavior toward minors

MEDIUM PRIORITY (review needed):
- Misinformation (false religious information, misquoted scripture)
- Privacy violations (sharing personal info without consent)
- Impersonation of pastors or church staff
- Theological disagreements that are escalating in tone

LOW PRIORITY (automated handling):
- Spam (repetitive content, promotional material)
- Off-topic posts
- Suspicious external links

Respond with JSON in this format:
{
  "flagged": boolean,
  "priority": "low" | "medium" | "high",
  "violations": ["array of specific violations found"],
  "reason": "primary violation category",
  "confidence": 0.0-1.0,
  "actionRequired": "none" | "review" | "hide" | "remove"
}

For HIGH priority: set actionRequired to "hide"
For MEDIUM priority: set actionRequired to "review" 
For LOW priority: set actionRequired to "review"
If no violations: set actionRequired to "none"
`;

    const response = await openai.chat.completions.create({
      model: "gpt-4o", // the newest OpenAI model is "gpt-4o" which was released May 13, 2024. do not change this unless explicitly requested by the user
      messages: [{ role: "user", content: prompt }],
      response_format: { type: "json_object" },
      max_tokens: 500,
      temperature: 0.3 // Lower temperature for consistent moderation decisions
    });

    const result = JSON.parse(response.choices[0].message.content || '{}');
    
    // Validate and sanitize the response
    return {
      flagged: Boolean(result.flagged),
      priority: ['low', 'medium', 'high', 'critical'].includes(result.priority) ? result.priority : 'medium',
      violations: Array.isArray(result.violations) ? result.violations : [],
      reason: typeof result.reason === 'string' ? result.reason : 'other',
      confidence: typeof result.confidence === 'number' ? Math.max(0, Math.min(1, result.confidence)) : 0.5,
      actionRequired: ['none', 'review', 'hide', 'remove'].includes(result.actionRequired) ? result.actionRequired : 'none'
    };

  } catch (error) {
    console.error('AI moderation analysis failed:', error);
    // Return safe fallback
    return {
      flagged: false,
      priority: 'medium',
      violations: [],
      reason: 'analysis_failed',
      confidence: 0,
      actionRequired: 'none'
    };
  }
}

export async function createAutoModerationReport(
  storage: any,
  contentType: string,
  contentId: number,
  moderationResult: ModerationResult,
  systemUserId: string = 'system'
): Promise<void> {
  if (!moderationResult.flagged) return;

  try {
    await storage.createContentReport({
      reporterId: systemUserId,
      contentType,
      contentId,
      reason: moderationResult.reason,
      description: `AI-detected violation: ${moderationResult.violations.join(', ')} (Confidence: ${(moderationResult.confidence * 100).toFixed(1)}%)`,
      priority: moderationResult.priority,
      isAutoGenerated: true
    });

    // Take immediate action for high priority violations
    if (moderationResult.actionRequired === 'hide' || moderationResult.actionRequired === 'remove') {
      await storage.hideContent(
        contentType, 
        contentId, 
        `Auto-moderation: ${moderationResult.reason} (${moderationResult.violations.join(', ')})`,
        systemUserId
      );
    }
  } catch (error) {
    console.error('Failed to create auto-moderation report:', error);
  }
}